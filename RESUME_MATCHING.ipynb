{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a9eebc-0234-4f74-b0da-d652346266bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\saray\\Anaconda3\\envs\\candmatch\\python.exe\n",
      "OPENROUTER_API_KEY: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Python:\", __import__('sys').executable)\n",
    "print(\"OPENROUTER_API_KEY:\", os.getenv(\"OPENROUTER_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d06a6f-31f8-449e-91d5-2694b1d6cdc0",
   "metadata": {},
   "source": [
    "Importing all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685ccf2d-7bd0-4933-a307-ece8f239bb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import os, io, re, json, time, hashlib\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "try:\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.pdfgen import canvas\n",
    "    from reportlab.lib.units import cm\n",
    "    import base64\n",
    "    REPORTLAB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    REPORTLAB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import pdfplumber\n",
    "except:\n",
    "    pdfplumber = None\n",
    "try:\n",
    "    import docx\n",
    "except:\n",
    "    docx = None\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "if not REPORTLAB_AVAILABLE:\n",
    "    print(\" Reportlab is  not installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b42997b-3f02-417c-bff2-bf1c5a9434f2",
   "metadata": {},
   "source": [
    "SETTING API KEYS AND MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7eaa60-7eac-44cb-880a-20a455dc4a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n"
     ]
    }
   ],
   "source": [
    "OPENROUTER_KEY = os.getenv(\"OPENROUTER_API_KEY\") #api key is set in enviornment\n",
    "OPENROUTER_EMBED_URL = \"https://openrouter.ai/api/v1/embeddings\"\n",
    "OPENROUTER_CHAT_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "EMBED_CANDIDATES = [\"openai/text-embedding-3-large\",\"openai/text-embedding-3-small\"]    #this model is used for embedding\n",
    "LLM_EXTRACT_MODEL = \"openai/gpt-3.5-turbo\"   #extract structured clusters\n",
    "CHAT_MODEL_DEFAULT = \"mistralai/mistral-7b-instruct\"  #used for generating explanations\n",
    "\n",
    "SEMANTIC_THRESHOLD = 0.75      #setting threshold to consider a skill as matched\n",
    "KEYWORD_BONUS_THRESHOLD = 0.7  \n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1337bb-1ce2-46f7-ba1e-25c33b995e0f",
   "metadata": {},
   "source": [
    "FILE PARSING,CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1473d560-1493-41d8-ae83-47ccce06f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File parsing and text cleaning functions done.\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_bytes(filename, b):  #extract contents if uploaded file is pdf, txt or docs\n",
    "    fname = (filename or \"\").lower()\n",
    "    if fname.endswith(\".txt\"):\n",
    "        return b.decode(\"utf-8\", errors=\"replace\")\n",
    "    if fname.endswith(\".pdf\") and pdfplumber:\n",
    "        try:\n",
    "            pages=[]\n",
    "            with pdfplumber.open(io.BytesIO(b)) as pdf:\n",
    "                for p in pdf.pages:\n",
    "                    pages.append(p.extract_text() or \"\")\n",
    "            return \"\\n\".join(pages)\n",
    "        except:\n",
    "            return \"\"\n",
    "  \n",
    "    if fname.endswith(\".docx\") and docx:\n",
    "        try:\n",
    "            doc = docx.Document(io.BytesIO(b))\n",
    "            return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "        except:\n",
    "            return \"\"\n",
    "    if fname.endswith(\".csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(io.BytesIO(b))\n",
    "            text_cols = [c for c in df.columns if 'text' in c.lower() or 'resume' in c.lower() or 'cv' in c.lower()]\n",
    "            if text_cols:\n",
    "                return \"\\n\\n\".join(df[text_cols[0]].astype(str).tolist())\n",
    "            return df.to_csv(index=False)\n",
    "        except:\n",
    "            return b.decode(\"utf-8\", errors=\"replace\")\n",
    "    try:\n",
    "        return b.decode(\"utf-8\", errors=\"replace\")\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def normalize_fileupload_value(val):\n",
    "    out=[]\n",
    "    if not val:\n",
    "        return out\n",
    "    if isinstance(val, dict):\n",
    "        for k,v in val.items():\n",
    "            if isinstance(v, dict) and 'content' in v:\n",
    "                name = v.get('name') or k\n",
    "                content = v.get('content')\n",
    "                if isinstance(content, (memoryview, bytearray)):\n",
    "                    content = bytes(content)\n",
    "                out.append({\"name\":name,\"content\":content})\n",
    "    elif isinstance(val, (list,tuple)):\n",
    "        for item in val:\n",
    "            if isinstance(item, dict) and 'content' in item:\n",
    "                name = item.get('name') or \"uploaded\"\n",
    "                content = item.get('content')\n",
    "                if isinstance(content, (memoryview, bytearray)):\n",
    "                    content = bytes(content)\n",
    "                out.append({\"name\":name,\"content\":content})\n",
    "    return out\n",
    "\n",
    "def sent_tokenize(text):\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = str(txt or \"\")\n",
    "    txt = txt.replace(\"\\x0c\",\" \")\n",
    "    txt = re.sub(r'\\(cid:\\d+\\)',' ', txt)\n",
    "    txt = re.sub(r'\\s+',' ', txt).strip()\n",
    "    return txt\n",
    "    \n",
    "print(\"File parsing and text cleaning functions done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40bfcce-10ad-4717-a823-2f49896882a1",
   "metadata": {},
   "source": [
    "SETTING OPENROUTER API COMMUNICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b346627c-579f-428f-884e-526bfe7a45e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter API functions defined.\n"
     ]
    }
   ],
   "source": [
    "def detect_working_openrouter_embedding_model(candidates=EMBED_CANDIDATES, test_text=\"hello\"):\n",
    "    if not OPENROUTER_KEY:\n",
    "        print(\"OPENROUTER_API_KEY not set in environment.\")\n",
    "        return None\n",
    "    headers = {\"Authorization\": f\"Bearer {OPENROUTER_KEY}\", \"Content-Type\":\"application/json\"}\n",
    "    for m in candidates:\n",
    "        payload = {\"model\": m, \"input\": [test_text]}\n",
    "        try:\n",
    "            resp = requests.post(OPENROUTER_EMBED_URL, headers=headers, json=payload, timeout=20)\n",
    "            if resp.status_code == 200:\n",
    "                j = resp.json()\n",
    "                if \"data\" in j and isinstance(j[\"data\"], list) and \"embedding\" in j[\"data\"][0]:\n",
    "                    return m\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_openrouter_embeddings(texts, model, batch_size=16):\n",
    "    if not OPENROUTER_KEY:\n",
    "        raise RuntimeError(\"OPENROUTER_API_KEY missing.\")\n",
    "    headers = {\"Authorization\": f\"Bearer {OPENROUTER_KEY}\", \"Content-Type\":\"application/json\"}\n",
    "    all_embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        payload = {\"model\": model, \"input\": batch}\n",
    "        try:\n",
    "            resp = requests.post(OPENROUTER_EMBED_URL, headers=headers, json=payload, timeout=60)\n",
    "            resp.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            try:\n",
    "                print(\"OpenRouter error:\", resp.status_code, resp.json())\n",
    "            except:\n",
    "                pass\n",
    "            raise RuntimeError(f\"OpenRouter embeddings failed: {e}\")\n",
    "            \n",
    "        j = resp.json()\n",
    "        for item in j[\"data\"]:\n",
    "            all_embs.append(np.array(item[\"embedding\"], dtype=np.float32))\n",
    "        time.sleep(0.05)\n",
    "    if not all_embs:\n",
    "        return np.zeros((0,0), dtype=np.float32)\n",
    "    arr = np.vstack(all_embs)\n",
    "    arr = arr / np.linalg.norm(arr, axis=1, keepdims=True)\n",
    "    return arr\n",
    "\n",
    "def openrouter_chat(prompt, model=CHAT_MODEL_DEFAULT, temperature=0.0, max_tokens=512):\n",
    "    if not OPENROUTER_KEY:\n",
    "        raise RuntimeError(\"OPENROUTER_API_KEY missing.\")\n",
    "    headers = {\"Authorization\": f\"Bearer {OPENROUTER_KEY}\", \"Content-Type\":\"application/json\"}\n",
    "    payload = {\"model\": model, \"messages\": [{\"role\":\"user\",\"content\": prompt}], \"temperature\": temperature, \"max_tokens\": max_tokens}\n",
    "    resp = requests.post(OPENROUTER_CHAT_URL, headers=headers, json=payload, timeout=120)\n",
    "    resp.raise_for_status()\n",
    "    j = resp.json()\n",
    "    return j[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    \n",
    "print(\"OpenRouter API functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a539fd6-a035-47ea-83aa-fb1eece09bc5",
   "metadata": {},
   "source": [
    "EXTRACTIONS AND MATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16484fd-8e1f-47cb-9365-794e513a51dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM extraction and Embedding matching done\n"
     ]
    }
   ],
   "source": [
    "def extract_requirement_clusters(job_text, model=LLM_EXTRACT_MODEL):   #specified that o/p should be in json in the prompt\n",
    "    prompt = (\n",
    "        \"You are a concise parser. Given a job description, return a JSON object with keys \"\n",
    "        \"\\\"must_have\\\", \\\"important\\\", \\\"nice_to_have\\\". Each value should be an array of short skill/competency phrases (1-5 words each). \"\n",
    "        \"It is CRITICAL that you extract the actual skills/competencies, not filler words like 'of' or 'the'. \"\n",
    "        \"If the job contains explicit labels like 'must have' or lists, follow them. Keep phrases short. Return only JSON.\\n\\n\"\n",
    "        f\"JOB_DESCRIPTION:\\n{job_text}\\n\\nReturn JSON now.\"\n",
    "    )\n",
    "    txt = openrouter_chat(prompt, model=model, temperature=0.0, max_tokens=500)\n",
    "    m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n",
    "    \n",
    "    if not m:\n",
    "        print(\"Warning: LLM failed to return structured JSON. Using token frequency fallback.\")\n",
    "        words = re.findall(r\"\\b[A-Za-z0-9\\-\\+#\\.\\_]+\\b\", job_text.lower())\n",
    "        common = sorted(set(words), key=lambda w: -words.count(w))[:12]\n",
    "        return {\"must_have\": common[:4], \"important\": common[4:8], \"nice_to_have\": common[8:12]}\n",
    "        \n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: LLM returned invalid JSON. Using token frequency fallback.\")\n",
    "        words = re.findall(r\"\\b[A-Za-z0-9\\-\\+#\\.\\_]+\\b\", job_text.lower())\n",
    "        common = sorted(set(words), key=lambda w: -words.count(w))[:12]\n",
    "        obj = {\"must_have\": common[:4], \"important\": common[4:8], \"nice_to_have\": common[8:12]}\n",
    "        \n",
    "    for k in (\"must_have\",\"important\",\"nice_to_have\"):\n",
    "        if k not in obj:\n",
    "            obj[k] = []\n",
    "        if isinstance(obj[k], list):\n",
    "            obj[k] = [p for p in obj[k] if len(p.split()) > 1 or p.lower() not in ['of', 'and', 'in', 'to', 'the', 'with', 'for', 'or']]\n",
    "            \n",
    "    return obj\n",
    "\n",
    "def compute_skill_matches_for_resume(resume_text, skill_phrases, embed_model, chunk_size=3):\n",
    "    out = {}\n",
    "    r_low = resume_text.lower()\n",
    "  \n",
    "    for p in skill_phrases:\n",
    "        p_low = p.lower()\n",
    "        kw_match = bool(re.search(r\"\\b\" + re.escape(p_low) + r\"\\b\", r_low))\n",
    "        out[p] = {\"kw_match\": kw_match, \"sim\": 0.0, \"best_evidence\": None}\n",
    "    \n",
    "    #chunking\n",
    "    sentences = sent_tokenize(resume_text)\n",
    "    chunks = [\" \".join(sentences[i:i+chunk_size]) for i in range(len(sentences)) if \" \".join(sentences[i:i+chunk_size])]\n",
    "    \n",
    "    if not chunks or not skill_phrases:\n",
    "        return out\n",
    "        \n",
    "    try:\n",
    "        texts = skill_phrases + chunks\n",
    "        embs = get_openrouter_embeddings(texts, model=embed_model, batch_size=8)\n",
    "        \n",
    "        skill_embs = embs[:len(skill_phrases)]\n",
    "        chunk_embs = embs[len(skill_phrases):]\n",
    "        \n",
    "        sims_matrix = (skill_embs @ chunk_embs.T).astype(float) \n",
    "        \n",
    "        for i, p in enumerate(skill_phrases):\n",
    "            max_sim = np.max(sims_matrix[i])\n",
    "            best_chunk_index = np.argmax(sims_matrix[i])\n",
    "            \n",
    "            out[p][\"sim\"] = float(max_sim)\n",
    "            out[p][\"best_evidence\"] = {\n",
    "                \"skill\": p,\n",
    "                \"chunk_text\": chunks[best_chunk_index],\n",
    "                \"score\": float(max_sim)\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during embedding computation: {e}\")\n",
    "        pass\n",
    "        \n",
    "    return out\n",
    "    \n",
    "print(\"LLM extraction and Embedding matching done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb7e9c-96b7-48f8-a165-fdad11c44a31",
   "metadata": {},
   "source": [
    "EXPLANATION AND RANKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24dbae24-50a5-444e-90bc-08c8e63e3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation and Ranking done.\n"
     ]
    }
   ],
   "source": [
    "def generate_candidate_comparator_explanation(candidateA, candidateB):\n",
    "    scoreA = candidateA.get('overall_score') if candidateA.get('overall_score') is not None else 0.0\n",
    "    scoreB = candidateB.get('overall_score') if candidateB.get('overall_score') is not None else 0.0\n",
    "\n",
    "    explanation = (\n",
    "        f\"CANDIDATE A (id:{candidateA.get('id')} name:{candidateA.get('name')} score:{scoreA:.3f}):\\n\"\n",
    "        f\"Matched must-have skills: {', '.join(candidateA.get('matched_skills',[]))}\\n\"\n",
    "        f\"Gaps: {', '.join(candidateA.get('gaps',[]))}\\n\"\n",
    "        f\"Top evidence snippets:\\n\"\n",
    "    )\n",
    "    for e in candidateA.get('top_evidence', []):\n",
    "        explanation += f\" - **{e.get('skill', 'Skill')}:** (Score {e.get('score', 0.0):.3f}) *{e['chunk_text'][:100]}...*\\n\"\n",
    "\n",
    "    explanation += f\"\\nCANDIDATE B (id:{candidateB.get('id')} name:{candidateB.get('name')} score:{scoreB:.3f}):\\n\"\n",
    "    explanation += f\"Matched must-have skills: {', '.join(candidateB.get('matched_skills',[]))}\\n\"\n",
    "    explanation += f\"Gaps: {', '.join(candidateB.get('gaps',[]))}\\n\"\n",
    "    explanation += f\"Top evidence snippets:\\n\"\n",
    "    for e in candidateB.get('top_evidence', []):\n",
    "        explanation += f\" - **{e.get('skill', 'Skill')}:** (Score {e.get('score', 0.0):.3f}) *{e['chunk_text'][:100]}...*\\n\"\n",
    "\n",
    "    if scoreA > scoreB:\n",
    "        explanation += \"\\nConclusion: Candidate A is stronger based on overall score.\\n\"\n",
    "    elif scoreB > scoreA:\n",
    "        explanation += \"\\nConclusion: Candidate B is stronger based on overall score.\\n\"\n",
    "    else:\n",
    "        explanation += \"\\nConclusion: Both candidates have equal scores.\\n\"\n",
    "\n",
    "    return explanation\n",
    "\n",
    "def explainable_ranking_pipeline(candidates):\n",
    "    for c in candidates:\n",
    "        if c.get('overall_score') is None:\n",
    "            c['overall_score'] = 0.0\n",
    "        if 'summary' not in c:\n",
    "            c['summary'] = \"No summary available.\"\n",
    "    sorted_candidates = sorted(candidates, key=lambda x: x['overall_score'], reverse=True)\n",
    "    explanations = []\n",
    "    \n",
    "    if len(sorted_candidates) >= 2:\n",
    "        for i in range(len(sorted_candidates) - 1):\n",
    "            a = sorted_candidates[i]\n",
    "            b = sorted_candidates[i + 1]\n",
    "            explanation = generate_candidate_comparator_explanation(a, b)\n",
    "            explanations.append(explanation)\n",
    "            \n",
    "    return explanations\n",
    "    \n",
    "print(\"Explanation and Ranking done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733e8e9-8932-4950-afa6-929b87bb46b0",
   "metadata": {},
   "source": [
    "INTEGRATING PDF OPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5894601d-8858-4e47-9abc-504b33f9e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF generation function done\n"
     ]
    }
   ],
   "source": [
    "def generate_pdf(job_text, requirements, candidates, explanations, filename=\"Candidate_Ranking.pdf\"):\n",
    "    if not REPORTLAB_AVAILABLE:\n",
    "        print(\"Reportlab not installed. Skipping PDF generation.\")\n",
    "        return\n",
    "        \n",
    "    buffer = io.BytesIO()\n",
    "    c = canvas.Canvas(buffer, pagesize=A4)\n",
    "    width, height = A4\n",
    "    margin = 2*cm\n",
    "    y = height - margin\n",
    "\n",
    "    def write_line(text, line_height=14, is_bold=False):\n",
    "        nonlocal y\n",
    "        if is_bold:\n",
    "             c.setFont(\"Helvetica-Bold\", line_height-4)\n",
    "        else:\n",
    "             c.setFont(\"Helvetica\", line_height-4)\n",
    "\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            max_width = width - 2 * margin\n",
    "            current_line = \"\"\n",
    "            for word in line.split(' '):\n",
    "                test_line = current_line + word + \" \"\n",
    "                if c.stringWidth(test_line) < max_width:\n",
    "                    current_line = test_line\n",
    "                else:\n",
    "                    c.drawString(margin, y, current_line.strip())\n",
    "                    y -= line_height\n",
    "                    current_line = word + \" \"\n",
    "                \n",
    "            if current_line.strip():\n",
    "                c.drawString(margin, y, current_line.strip())\n",
    "                y -= line_height\n",
    "                \n",
    "            if y < margin:\n",
    "                c.showPage()\n",
    "                y = height - margin\n",
    "        c.setFont(\"Helvetica\", 12)\n",
    "\n",
    "    c.setFont(\"Helvetica-Bold\", 16)\n",
    "    write_line(\"Candidate Matching Report\", line_height=20, is_bold=True)\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    write_line(\"\")\n",
    "    write_line(\"Job Description:\", 16, is_bold=True)\n",
    "    write_line(\"\")\n",
    "    write_line(\"Extracted Requirement Clusters:\", 16, is_bold=True)\n",
    "    for k,v in requirements.items():\n",
    "        write_line(f\"{k}: {', '.join(v)}\")\n",
    "\n",
    "    write_line(\"\")\n",
    "    write_line(\"Candidate Comparisons:\", 16, is_bold=True)\n",
    "    for exp in explanations:\n",
    "        write_line(exp)\n",
    "        write_line(\"-\"*50)\n",
    "\n",
    "    write_line(\"\")\n",
    "    write_line(\"Ranking Summary:\", 16, is_bold=True)\n",
    "    for cnd in sorted(candidates, key=lambda x: x['overall_score'], reverse=True):\n",
    "        write_line(f\"RANK {cnd['name']} - Score: {cnd['overall_score']:.3f}\")\n",
    "        write_line(f\"Matched Skills: {', '.join(cnd['matched_skills'])}\")\n",
    "        write_line(f\"Gaps: {', '.join(cnd['gaps'])}\")\n",
    "        write_line(\"-\" * 20)\n",
    "\n",
    "\n",
    "    c.save()\n",
    "    buffer.seek(0)\n",
    "    b64 = base64.b64encode(buffer.read()).decode()\n",
    "    href = f'<a download=\"{filename}\" href=\"data:application/pdf;base64,{b64}\">Download PDF Report</a>'\n",
    "    display(HTML(href))\n",
    "\n",
    "print(\"PDF generation function done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a56fb09b-66cc-48ce-ac69-30bea8da903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main processing logic defined.\n"
     ]
    }
   ],
   "source": [
    "def on_process_click(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        job_text = job_textarea.value.strip()\n",
    "        if not job_text and job_upload.value:\n",
    "            files = normalize_fileupload_value(job_upload.value)\n",
    "            if files:\n",
    "                job_text = extract_text_from_bytes(files[0]['name'], files[0]['content'])\n",
    "        if not job_text:\n",
    "            print(\" Please provide a job description (paste or upload).\")\n",
    "            return\n",
    "        job_text = clean_text(job_text)\n",
    "        print(\" Job description loaded.\")\n",
    "\n",
    "        print(f\" Extracting job requirements using {LLM_EXTRACT_MODEL}...\")\n",
    "        requirements = extract_requirement_clusters(job_text, model=LLM_EXTRACT_MODEL)\n",
    "        \n",
    "        if not any(requirements.values()):\n",
    "             print(\"Extraction failed. Please check OpenRouter key or model availability.\")\n",
    "             return\n",
    "             \n",
    "        all_skills = requirements[\"must_have\"] + requirements[\"important\"] + requirements[\"nice_to_have\"]\n",
    "        print(\"Extracted requirement clusters:\", requirements)\n",
    "\n",
    "        print(\" Detecting working embedding model...\")\n",
    "        embed_model = detect_working_openrouter_embedding_model()\n",
    "        if not embed_model:\n",
    "            print(\" Could not find a working embedding model. Check your OPENROUTER_API_KEY.\")\n",
    "            return\n",
    "        print(\"Using embedding model:\", embed_model)\n",
    "        files = normalize_fileupload_value(resume_upload.value)\n",
    "        if not files:\n",
    "            print(\" Please upload at least one resume file.\")\n",
    "            return\n",
    "        candidates = []\n",
    "        print(f\" Processing {len(files)} resumes...\")\n",
    "        for idx, f in enumerate(files):\n",
    "            print(f\"  -> Processing {f['name']}...\")\n",
    "            txt = extract_text_from_bytes(f['name'], f['content'])\n",
    "            txt = clean_text(txt)\n",
    "            \n",
    "            skill_matches = compute_skill_matches_for_resume(txt, all_skills, embed_model)\n",
    "            \n",
    "\n",
    "            overall_score = 0.0\n",
    "            for skill_phrase, v in skill_matches.items():\n",
    "                sim_score = v['sim']\n",
    "                weight = 1.0 \n",
    "                if skill_phrase in requirements['must_have']:\n",
    "                    weight = 4.0 # High priority\n",
    "                elif skill_phrase in requirements['important']:\n",
    "                    weight = 1.5 # Medium priority\n",
    "\n",
    "                final_score = sim_score * weight\n",
    "             \n",
    "                if v['kw_match'] and sim_score >= KEYWORD_BONUS_THRESHOLD:\n",
    "                     final_score += 0.5 \n",
    "                \n",
    "                if sim_score < 0.7:\n",
    "                    final_score *= 0.1\n",
    "                    \n",
    "                overall_score += final_score\n",
    "          \n",
    "            matched_skills = [k for k,v in skill_matches.items() if v['kw_match'] or v['sim'] > SEMANTIC_THRESHOLD]\n",
    "            gaps = [k for k in requirements['must_have'] if k not in matched_skills]\n",
    "            \n",
    "            all_evidence = [v['best_evidence'] for v in skill_matches.values() if v['best_evidence'] is not None and v['sim'] > SEMANTIC_THRESHOLD]\n",
    "            top_evidence = sorted(all_evidence, key=lambda x: x['score'], reverse=True)[:3]\n",
    "\n",
    "            candidates.append({\n",
    "                \"id\": idx+1,\n",
    "                \"name\": f['name'],\n",
    "                \"text\": txt,\n",
    "                \"overall_score\": overall_score,\n",
    "                \"matched_skills\": matched_skills,\n",
    "                \"gaps\": gaps,\n",
    "                \"top_evidence\": top_evidence \n",
    "            })\n",
    "\n",
    "        print(\"Generating explainable comparisons...\")\n",
    "        explanations = explainable_ranking_pipeline(candidates)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Generated Explanations (Comparator Reports)\")\n",
    "        print(\"=\"*60)\n",
    "        for exp in explanations:\n",
    "            print(exp)\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        print(\"\\n Ranking summary:\")\n",
    "        sorted_cands = sorted(candidates, key=lambda x: x['overall_score'], reverse=True)\n",
    "        for c in sorted_cands:\n",
    "            print(f\"{c['name']} - Score: {c['overall_score']:.3f}, Matched Skills: {', '.join(c['matched_skills'])}\")\n",
    "\n",
    "        \n",
    "        if REPORTLAB_AVAILABLE:\n",
    "            try:\n",
    "                generate_pdf(job_text, requirements, candidates, explanations)\n",
    "                print(\"\\nPDF report generated! Click the link above to download.\")\n",
    "            except Exception as e:\n",
    "                print(\"Failed to generate PDF:\", e)\n",
    "        else:\n",
    "            print(\"\\nNOTE: Reportlab library is missing. PDF report generation skipped.\")\n",
    "\n",
    "print(\"Main processing logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fdcae-13b2-47d2-ae02-b4ac868cf588",
   "metadata": {},
   "source": [
    "UI SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d53b0d-e4ea-4e0e-87fe-28018f88d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Widgets created.\n"
     ]
    }
   ],
   "source": [
    "job_textarea = widgets.Textarea(value=\"\", placeholder=\"Paste job description (required)\", description=\"Job text:\", layout=widgets.Layout(width=\"100%\", height=\"140px\"))\n",
    "job_upload = widgets.FileUpload(accept=\".txt,.pdf,.docx,.csv,.json\", multiple=False, description=\"Upload job file (optional)\")\n",
    "resume_upload = widgets.FileUpload(accept=\".txt,.pdf,.docx,.csv,.json\", multiple=True, description=\"Upload resume(s)\")\n",
    "process_btn = widgets.Button(description=\"Process & Explain\", button_style=\"success\")\n",
    "output = widgets.Output()\n",
    "\n",
    "print(\"Widgets created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e1e25-9431-459b-af80-95a6cf6431e9",
   "metadata": {},
   "source": [
    "UI DISPLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cf4d146-dca9-46d7-aa57-ea68cdf42a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf978894be843108163903425f05dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Paste job description OR upload file. Then upload one or more resumes (txt/pdf/docâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive UI displayed. Click 'Process & Explain' after uploading files.\n"
     ]
    }
   ],
   "source": [
    "process_btn.on_click(on_process_click)\n",
    "display(widgets.VBox([\n",
    "    widgets.Label(\"Paste job description OR upload file. Then upload one or more resumes (txt/pdf/docx/csv/json).\"),\n",
    "    job_textarea, job_upload, widgets.HTML(\"<hr>\"), resume_upload, process_btn, output\n",
    "]))\n",
    "\n",
    "print(\"Interactive UI displayed. Click 'Process & Explain' after uploading files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e4101-27a8-454f-8574-31717be6e0a2",
   "metadata": {},
   "source": [
    "ENTIRE PROJECT STREAMLIT DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056392b0-6d0b-4a02-b7b6-769b04ac8b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import os, io, re, json, time, hashlib\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    import pdfplumber\n",
    "except ImportError:\n",
    "    pdfplumber = None\n",
    "try:\n",
    "    import docx\n",
    "except ImportError:\n",
    "    docx = None\n",
    "\n",
    "try:\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.pdfgen import canvas\n",
    "    from reportlab.lib.units import cm\n",
    "    import base64\n",
    "    REPORTLAB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    REPORTLAB_AVAILABLE = False\n",
    "\n",
    "\n",
    "OPENROUTER_EMBED_URL = \"https://openrouter.ai/api/v1/embeddings\"\n",
    "OPENROUTER_CHAT_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "EMBED_CANDIDATES = [\"openai/text-embedding-3-large\",\"openai/text-embedding-3-small\"]\n",
    "LLM_EXTRACT_MODEL = \"openai/gpt-3.5-turbo\" \n",
    "CHAT_MODEL_DEFAULT = \"mistralai/mistral-7b-instruct\"\n",
    "\n",
    "\n",
    "SEMANTIC_THRESHOLD = 0.75     \n",
    "KEYWORD_BONUS_THRESHOLD = 0.7  \n",
    "\n",
    "@st.cache_data\n",
    "def extract_text_from_uploaded_file(uploaded_file):\n",
    "    \"\"\"Extracts text from various file types supported by Streamlit UploadedFile.\"\"\"\n",
    "    filename = uploaded_file.name\n",
    "    file_bytes = uploaded_file.getvalue()\n",
    "    \n",
    "    fname = filename.lower()\n",
    "\n",
    "    if fname.endswith(\".txt\"):\n",
    "        return file_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    if fname.endswith(\".pdf\") and pdfplumber:\n",
    "        try:\n",
    "            pages=[]\n",
    "            with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:\n",
    "                for p in pdf.pages:\n",
    "                    pages.append(p.extract_text() or \"\")\n",
    "            return \"\\n\".join(pages)\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "            \n",
    "  \n",
    "    if fname.endswith(\".docx\") and docx:\n",
    "        try:\n",
    "            doc = docx.Document(io.BytesIO(file_bytes))\n",
    "            return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    try:\n",
    "        return file_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def sent_tokenize(text):\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sents if s.strip()]\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = str(txt or \"\")\n",
    "    txt = txt.replace(\"\\x0c\",\" \")\n",
    "    txt = re.sub(r'\\(cid:\\d+\\)',' ', txt)\n",
    "    txt = re.sub(r'\\s+',' ', txt).strip()\n",
    "    return txt\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Generating embeddings via OpenRouter...\")\n",
    "def get_openrouter_embeddings(texts, model, api_key, batch_size=16):\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENROUTER_API_KEY missing.\")\n",
    "    \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\":\"application/json\"}\n",
    "    all_embs = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        payload = {\"model\": model, \"input\": batch}\n",
    "        \n",
    "        try:\n",
    "            resp = requests.post(OPENROUTER_EMBED_URL, headers=headers, json=payload, timeout=60)\n",
    "            resp.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            st.error(f\"OpenRouter Embeddings Failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        j = resp.json()\n",
    "        for item in j[\"data\"]:\n",
    "            all_embs.append(np.array(item[\"embedding\"], dtype=np.float32))\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    if not all_embs:\n",
    "        return np.zeros((0,0), dtype=np.float32)\n",
    "        \n",
    "    arr = np.vstack(all_embs)\n",
    "    arr = arr / np.linalg.norm(arr, axis=1, keepdims=True)\n",
    "    return arr\n",
    "\n",
    "\n",
    "@st.cache_data(show_spinner=\"Extracting requirements via OpenRouter...\")\n",
    "def openrouter_chat(prompt, model, api_key, temperature=0.0, max_tokens=512):\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENROUTER_API_KEY missing.\")\n",
    "        \n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\":\"application/json\"}\n",
    "    payload = {\"model\": model, \"messages\": [{\"role\":\"user\",\"content\": prompt}], \"temperature\": temperature, \"max_tokens\": max_tokens}\n",
    "    \n",
    "    try:\n",
    "        resp = requests.post(OPENROUTER_CHAT_URL, headers=headers, json=payload, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        st.error(f\"OpenRouter Chat Failed: {e}\")\n",
    "        raise\n",
    "        \n",
    "    j = resp.json()\n",
    "    return j[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "\n",
    "\n",
    "def extract_requirement_clusters(job_text, api_key, model=LLM_EXTRACT_MODEL):\n",
    "    prompt = (\n",
    "        \"You are a concise parser. Given a job description, return a JSON object with keys \"\n",
    "        \"\\\"must_have\\\", \\\"important\\\", \\\"nice_to_have\\\". Each value should be an array of short skill/competency phrases (1-5 words each). \"\n",
    "        \"It is CRITICAL that you extract the actual skills/competencies, not filler words like 'of' or 'the'. \"\n",
    "        \"If the job contains explicit labels like 'must have' or lists, follow them. Keep phrases short. Return only JSON.\\n\\n\"\n",
    "        f\"JOB_DESCRIPTION:\\n{job_text}\\n\\nReturn JSON now.\"\n",
    "    )\n",
    "    txt = openrouter_chat(prompt, model=model, api_key=api_key, temperature=0.0, max_tokens=500)\n",
    "    m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n",
    "    \n",
    "    if not m:\n",
    "        st.warning(\"LLM failed to return structured JSON. Using token frequency fallback.\")\n",
    "        words = re.findall(r\"\\b[A-Za-z0-9\\-\\+#\\.\\_]+\\b\", job_text.lower())\n",
    "        common = sorted(set(words), key=lambda w: -words.count(w))[:12]\n",
    "        return {\"must_have\": common[:4], \"important\": common[4:8], \"nice_to_have\": common[8:12]}\n",
    "        \n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        st.warning(\"LLM returned invalid JSON. Using token frequency fallback.\")\n",
    "        words = re.findall(r\"\\b[A-Za-z0-9\\-\\+#\\.\\_]+\\b\", job_text.lower())\n",
    "        common = sorted(set(words), key=lambda w: -words.count(w))[:12]\n",
    "        obj = {\"must_have\": common[:4], \"important\": common[4:8], \"nice_to_have\": common[8:12]}\n",
    "        \n",
    "    for k in (\"must_have\",\"important\",\"nice_to_have\"):\n",
    "        if k not in obj:\n",
    "            obj[k] = []\n",
    "        if isinstance(obj[k], list):\n",
    "            # Clean up short common words\n",
    "            obj[k] = [p.strip() for p in obj[k] if len(p.split()) > 1 or p.lower() not in ['of', 'and', 'in', 'to', 'the', 'with', 'for', 'or']]\n",
    "            \n",
    "    return obj\n",
    "\n",
    "def compute_skill_matches_for_resume(resume_text, skill_phrases, embed_model, api_key, chunk_size=3):\n",
    "    out = {}\n",
    "    r_low = resume_text.lower()\n",
    "    \n",
    "    for p in skill_phrases:\n",
    "        p_low = p.lower()\n",
    "        kw_match = bool(re.search(r\"\\b\" + re.escape(p_low) + r\"\\b\", r_low))\n",
    "        out[p] = {\"kw_match\": kw_match, \"sim\": 0.0, \"best_evidence\": None}\n",
    "    \n",
    "    sentences = sent_tokenize(resume_text)\n",
    "    chunks = [\" \".join(sentences[i:i+chunk_size]) for i in range(len(sentences)) if \" \".join(sentences[i:i+chunk_size])]\n",
    "    \n",
    "    if not chunks or not skill_phrases:\n",
    "        return out\n",
    "        \n",
    "    try:\n",
    "        texts = skill_phrases + chunks\n",
    "        embs = get_openrouter_embeddings(texts, model=embed_model, api_key=api_key, batch_size=8)\n",
    "        \n",
    "        skill_embs = embs[:len(skill_phrases)]\n",
    "        chunk_embs = embs[len(skill_phrases):]\n",
    "        \n",
    "        sims_matrix = (skill_embs @ chunk_embs.T).astype(float) \n",
    "        \n",
    "        for i, p in enumerate(skill_phrases):\n",
    "            max_sim = np.max(sims_matrix[i])\n",
    "            best_chunk_index = np.argmax(sims_matrix[i])\n",
    "            \n",
    "            out[p][\"sim\"] = float(max_sim)\n",
    "            out[p][\"best_evidence\"] = {\n",
    "                \"skill\": p,\n",
    "                \"chunk_text\": chunks[best_chunk_index],\n",
    "                \"score\": float(max_sim)\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error during embedding computation: {e}\")\n",
    "        pass\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def generate_candidate_comparator_explanation(candidateA, candidateB):\n",
    "    scoreA = candidateA.get('overall_score', 0.0)\n",
    "    scoreB = candidateB.get('overall_score', 0.0)\n",
    "\n",
    "    explanation = (\n",
    "        f\"**CANDIDATE A** (Name: **{candidateA.get('name')}** | Score: **{scoreA:.3f}**):\\n\"\n",
    "        f\"Matched must-have skills: {', '.join(candidateA.get('matched_skills',[]))}\\n\"\n",
    "        f\"Gaps: {', '.join(candidateA.get('gaps',[]))}\\n\"\n",
    "        f\"Top evidence snippets:\\n\"\n",
    "    )\n",
    "    for e in candidateA.get('top_evidence', []):\n",
    "        explanation += f\" - **{e.get('skill', 'Skill')}:** (Score {e.get('score', 0.0):.3f}) *{e['chunk_text'][:100]}...*\\n\"\n",
    "\n",
    "    explanation += f\"\\n**CANDIDATE B** (Name: **{candidateB.get('name')}** | Score: **{scoreB:.3f}**):\\n\"\n",
    "    explanation += f\"Matched must-have skills: {', '.join(candidateB.get('matched_skills',[]))}\\n\"\n",
    "    explanation += f\"Gaps: {', '.join(candidateB.get('gaps',[]))}\\n\"\n",
    "    explanation += f\"Top evidence snippets:\\n\"\n",
    "    for e in candidateB.get('top_evidence', []):\n",
    "        explanation += f\" - **{e.get('skill', 'Skill')}:** (Score {e.get('score', 0.0):.3f}) *{e['chunk_text'][:100]}...*\\n\"\n",
    "\n",
    "    if scoreA > scoreB:\n",
    "        explanation += \"\\n**Conclusion: Candidate A is stronger based on overall score.**\\n\"\n",
    "    elif scoreB > scoreA:\n",
    "        explanation += \"\\n**Conclusion: Candidate B is stronger based on overall score.**\\n\"\n",
    "    else:\n",
    "        explanation += \"\\n**Conclusion: Both candidates have equal scores.**\\n\"\n",
    "\n",
    "    return explanation\n",
    "\n",
    "def explainable_ranking_pipeline(candidates):\n",
    "    sorted_candidates = sorted(candidates, key=lambda x: x['overall_score'], reverse=True)\n",
    "    explanations = []\n",
    "    \n",
    "    if len(sorted_candidates) >= 2:\n",
    "        for i in range(len(sorted_candidates) - 1):\n",
    "            a = sorted_candidates[i]\n",
    "            b = sorted_candidates[i + 1]\n",
    "            explanation = generate_candidate_comparator_explanation(a, b)\n",
    "            explanations.append(explanation)\n",
    "            \n",
    "    return explanations, sorted_candidates\n",
    "\n",
    "\n",
    "def generate_pdf_report(job_text, requirements, candidates, explanations):\n",
    "    if not REPORTLAB_AVAILABLE:\n",
    "        st.warning(\"Reportlab library not found. PDF report generation skipped.\")\n",
    "        return None\n",
    "        \n",
    "    buffer = io.BytesIO()\n",
    "    c = canvas.Canvas(buffer, pagesize=A4)\n",
    "    width, height = A4\n",
    "    margin = 2*cm\n",
    "    y = height - margin\n",
    "\n",
    "    def write_line(text, line_height=14, is_bold=False):\n",
    "        nonlocal y\n",
    "        if y < margin + line_height * 2: # Check if a new page is needed soon\n",
    "            c.showPage()\n",
    "            y = height - margin\n",
    "        \n",
    "        if is_bold:\n",
    "             c.setFont(\"Helvetica-Bold\", line_height-4)\n",
    "        else:\n",
    "             c.setFont(\"Helvetica\", line_height-4)\n",
    "\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            max_width = width - 2 * margin\n",
    "            current_line = \"\"\n",
    "            for word in line.split(' '):\n",
    "                test_line = current_line + word + \" \"\n",
    "                if c.stringWidth(test_line) < max_width:\n",
    "                    current_line = test_line\n",
    "                else:\n",
    "                    c.drawString(margin, y, current_line.strip())\n",
    "                    y -= line_height\n",
    "                    current_line = word + \" \"\n",
    "            \n",
    "            if current_line.strip():\n",
    "                c.drawString(margin, y, current_line.strip())\n",
    "                y -= line_height\n",
    "                \n",
    "        c.setFont(\"Helvetica\", 12)\n",
    "        return y \n",
    "\n",
    "\n",
    "    c.setFont(\"Helvetica-Bold\", 16)\n",
    "    write_line(\"Candidate Matching Report\", line_height=20, is_bold=True)\n",
    "    write_line(f\"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\", is_bold=False)\n",
    "\n",
    "    # Job description\n",
    "    write_line(\"Job Description Summary:\", 16, is_bold=True)\n",
    "    write_line(job_text[:500] + (\"...\" if len(job_text) > 500 else \"\"), 12)\n",
    "    \n",
    "    write_line(\"\")\n",
    "    write_line(\"Extracted Requirement Clusters:\", 16, is_bold=True)\n",
    "    for k,v in requirements.items():\n",
    "        write_line(f\"**{k.replace('_', ' ').title()}:** {', '.join(v)}\", 12)\n",
    "\n",
    "    write_line(\"\")\n",
    "    write_line(\"Candidate Comparisons:\", 16, is_bold=True)\n",
    "    for exp in explanations:\n",
    "        write_line(exp.replace('**', '').replace('\\n', '\\n')) # Remove markdown for PDF text\n",
    "        write_line(\"-\" * 50)\n",
    "\n",
    "    write_line(\"\")\n",
    "    write_line(\"Ranking Summary:\", 16, is_bold=True)\n",
    "    for cnd in sorted(candidates, key=lambda x: x['overall_score'], reverse=True):\n",
    "        write_line(f\"RANK {cnd['name']} - Score: {cnd['overall_score']:.3f}\")\n",
    "        write_line(f\"Matched Skills: {', '.join(cnd['matched_skills'])}\")\n",
    "        write_line(f\"Gaps: {', '.join(cnd['gaps'])}\")\n",
    "        write_line(\"-\" * 20)\n",
    "\n",
    "    c.save()\n",
    "    buffer.seek(0)\n",
    "    return buffer\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Resume Matcher\", layout=\"wide\")\n",
    "    st.title(\"ðŸ“„ Semantic Resume Matcher and Ranker\")\n",
    "    st.markdown(\"Use OpenRouter (OpenAI/Mistral) embeddings to compare resumes against a job description. The system heavily weights 'Must-Have' skills.\")\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "   \n",
    "    st.sidebar.header(\"API Configuration\")\n",
    "    openrouter_api_key = st.sidebar.text_input(\n",
    "        \"OpenRouter API Key\",\n",
    "        type=\"password\",\n",
    "        value=os.getenv(\"OPENROUTER_API_KEY\", \"\") # Fallback to env var\n",
    "    )\n",
    "    if not openrouter_api_key:\n",
    "        st.sidebar.error(\"Please enter your OpenRouter API Key.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    st.header(\"1. Job Description (JD)\")\n",
    "    jd_tab, jd_file_tab = st.tabs([\"Paste Text\", \"Upload File\"])\n",
    "    \n",
    "    with jd_tab:\n",
    "        job_text = st.text_area(\n",
    "            \"Paste Job Description Text here:\",\n",
    "            height=200,\n",
    "            key=\"jd_text_area\"\n",
    "        )\n",
    "    with jd_file_tab:\n",
    "        jd_file = st.file_uploader(\"Upload Job Description File (.txt, .pdf, .docx)\", type=[\"txt\", \"pdf\", \"docx\"], key=\"jd_file_uploader\")\n",
    "        if jd_file:\n",
    "            job_text = extract_text_from_uploaded_file(jd_file)\n",
    "\n",
    "    if not job_text:\n",
    "        st.info(\"Please input or upload a Job Description to proceed.\")\n",
    "        return\n",
    "\n",
    "    st.header(\"2. Resumes\")\n",
    "    resume_files = st.file_uploader(\n",
    "        \"Upload Candidate Resumes (.pdf, .docx, .txt)\",\n",
    "        type=[\"pdf\", \"docx\", \"txt\"],\n",
    "        accept_multiple_files=True\n",
    "    )\n",
    "\n",
    "    if not resume_files:\n",
    "        st.info(\"Please upload at least one resume.\")\n",
    "        return\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    if st.button(\" Process Candidates Resume\"):\n",
    "        \n",
    "       \n",
    "        st.cache_data.clear()\n",
    "        \n",
    "        try:\n",
    "      \n",
    "            cleaned_job_text = clean_text(job_text)\n",
    "            st.success(\"Job Description loaded and cleaned.\")\n",
    "\n",
    "            requirements = extract_requirement_clusters(cleaned_job_text, openrouter_api_key, model=LLM_EXTRACT_MODEL)\n",
    "            all_skills = requirements[\"must_have\"] + requirements[\"important\"] + requirements[\"nice_to_have\"]\n",
    "            st.subheader(\"Extracted Requirements:\")\n",
    "            st.json(requirements)\n",
    "\n",
    "            candidates = []\n",
    "            resume_progress = st.progress(0, text=\"Processing resumes...\")\n",
    "            \n",
    "            for idx, f in enumerate(resume_files):\n",
    "                st.info(f\"Processing **{f.name}**...\")\n",
    "                \n",
    "                resume_text = extract_text_from_uploaded_file(f)\n",
    "                cleaned_resume_text = clean_text(resume_text)\n",
    "\n",
    "                skill_matches = compute_skill_matches_for_resume(cleaned_resume_text, all_skills, EMBED_CANDIDATES[0], openrouter_api_key)\n",
    "                \n",
    "                overall_score = 0.0\n",
    "                for skill_phrase, v in skill_matches.items():\n",
    "                    sim_score = v['sim']\n",
    "                    weight = 1.0 \n",
    "                    if skill_phrase in requirements['must_have']:\n",
    "                        weight = 4.0 \n",
    "                    elif skill_phrase in requirements['important']:\n",
    "                        weight = 1.5 \n",
    "\n",
    "                    final_score = sim_score * weight\n",
    "                    \n",
    "                    if v['kw_match'] and sim_score >= KEYWORD_BONUS_THRESHOLD:\n",
    "                         final_score += 0.5 \n",
    "                    \n",
    "                    if sim_score < 0.7:\n",
    "                        final_score *= 0.1\n",
    "                        \n",
    "                    overall_score += final_score\n",
    "        \n",
    "                matched_skills = [k for k,v in skill_matches.items() if v['kw_match'] or v['sim'] > SEMANTIC_THRESHOLD]\n",
    "                gaps = [k for k in requirements['must_have'] if k not in matched_skills]\n",
    "                \n",
    "                all_evidence = [v['best_evidence'] for v in skill_matches.values() if v['best_evidence'] is not None and v['sim'] > SEMANTIC_THRESHOLD]\n",
    "                top_evidence = sorted(all_evidence, key=lambda x: x['score'], reverse=True)[:3]\n",
    "\n",
    "                candidates.append({\n",
    "                    \"id\": idx+1,\n",
    "                    \"name\": f.name,\n",
    "                    \"overall_score\": overall_score,\n",
    "                    \"matched_skills\": matched_skills,\n",
    "                    \"gaps\": gaps,\n",
    "                    \"top_evidence\": top_evidence \n",
    "                })\n",
    "                resume_progress.progress((idx + 1) / len(resume_files), text=f\"Processed **{f.name}**\")\n",
    "\n",
    "            resume_progress.empty()\n",
    "\n",
    "            explanations, sorted_candidates = explainable_ranking_pipeline(candidates)\n",
    "\n",
    "            st.header(\"3. Ranking Summary \")\n",
    "        \n",
    "            ranking_data = [\n",
    "                {\"Rank\": i+1, \"Candidate\": c['name'], \"Score\": f\"{c['overall_score']:.3f}\", \"Matched Skills\": \", \".join(c['matched_skills'])}\n",
    "                for i, c in enumerate(sorted_candidates)\n",
    "            ]\n",
    "            st.dataframe(ranking_data, use_container_width=True, hide_index=True)\n",
    "            \n",
    "            st.header(\"4. Head-to-Head Comparisons\")\n",
    "            for exp in explanations:\n",
    "                st.markdown(exp)\n",
    "                st.markdown(\"---\")\n",
    "\n",
    "            pdf_buffer = generate_pdf_report(cleaned_job_text, requirements, candidates, explanations)\n",
    "            if pdf_buffer:\n",
    "                st.download_button(\n",
    "                    label=\"Download PDF Report\",\n",
    "                    data=pdf_buffer,\n",
    "                    file_name=\"candidate_ranking_report.pdf\",\n",
    "                    mime=\"application/pdf\"\n",
    "                )\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            st.error(f\"A critical error occurred: {e}. Please check your API key and network connection.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b8048-904d-48a0-9f16-378f62b226b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (candmatch)",
   "language": "python",
   "name": "candmatch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
